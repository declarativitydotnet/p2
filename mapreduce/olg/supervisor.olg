//materialize(env, infinity,infinity, keys(2)).


#define MAP_DF_STAGE "dataflow Map0{ Mux -> MergeSort ->DMux}"
#define REDUCE_DF_STAGE "dataflow Reduce0 { Mux -> MergeSort -> DMux}"

#define SPLIT_RULES "materialize(tTable, infinity,infinity, keys(2)).  a0 onResp(@Y,MSG) :- tTable(@X,MSG), Y := grumpy.berkeley.intel-research.net:30000."

#define MSORT_DF "edit P2 { let MSort = MergeSort(\"MSort0\",1,1);.dDemux[\"sort\"] -> MSort -> [+].dRoundRobin;}."

i00	deployStageDF(@X,Y,DF) :- periodic(@X,E,0,1), Y := "10.212.2.61:8888", DF := MSORT_DF.

//create job map/reduce flow
//	instanciate the specific element (ship p2dl tuples)
watch(deployStageDF).
ie0	script(@Y,X,DF) :- deployStageDF(@X,Y, DF).

//	install the map/reduce rules (ship overlog tuples)
imr0	overlogEvent(@Y,X,NAME,PROG) :- deploystageol(@X, Y, NAME, PROG).

//	start feeding input

//phase transition checks: make sure all map/reduce stages in current job have finished

//heart beat failure detectors for all nodes with auto-restart
