This document outlines the steps to running P2 on planetlab, and visualizing
events that occur on nodes. A planetlab installation requires the following
4 rpms: boost-1.32.0-6.i386.rpm, libstdc++-4.0.0-8.i386.rpm, python-2.3.4-11.i386.rpm,
and p2-*.rpm. The last rpm is the P2 rpm, which you will have to build under a FC2-3 
environment (i.e., grouchy or grumpy).


Steps to build P2 rpm:
1. Log into a machine running RedHat version no later than FC3.
2. Ensure you have the following directory structure under your 
home directory:
/homes/<login>/rpm
	/BUILD  
	/RPMS  
		/i386
	/SOURCES  
	/SPECS  
	/SRPMS

3. Create a file named '.rpmmacros' in your home directory containing the
text below:
%_topdir /homes/<login>/rpm
%__strip /bin/true
%__os_install_post    \
    /usr/lib/rpm/redhat/brp-compress \
%{nil}

4. If necessary, check out P2 from the cvs repository and do the 'setup' 
and configure steps.
5. Make a P2 distribution tarball: 'make dist'
6. Take the distribution file (e.g., p2-0.7.2.tar.gz) and place it in
the '/homes/<login>/rpm/SOURCES' directory created in step 2. 
7. In the top level P2 directory ('./phi/phi/') make sure you see a file
titled p2.spec. Edit this file and ensure the 'Version' line matches the
version indicated on the distribution file (e.g., '0.7.2'). 
8. Execute: 'rpmbuild -ba p2.spec'
9. If step 8 was successfull you will find the P2 rpm in the 
'/homes/<login>/rpm/RPMS/i386' directory. Place this rpm in the
'phi/phi/rpms' directory. You should now be ready to run the planetlab
setup script (psetup.py).


=============================================================
This section describes the the PlanetLab setup script 'psetup.py'.
The script assumes that the 4 aforementioned rpms are in the 
'phi/phi/rpms' directory. In order to run this script you must have
no less that 'user' priviledge on the slice in which you wish to 
run P2. You should edit the psetup.py script and make sure the file
names to the rpms match that which is indicated the arrays at the
beginning of the script.  The usage statement to psetup.py is as 
follows:

Usage: psetup.py [-d] [-a] [-t pingCheckPort] [-j <parallel>] [-(i|I)] [-k] \
                 [-Dvar=<value> [-Dvar=<value> [...]] -o <overlog>] \
                 [-m master_addr -p master_port] \
                 -n <slice_name> [<planetLabNode>]

Description of arguments:
-n	This argument must always be given. It indicates the planetlab slice
	on which the command is to be executed.
-d	This will deallocate all planetlab nodes from the indicated slice.
-a 	This will add all available nodes to the slice.
-t 	This is a port number (e.g., 10002) that will be used to ensure
	an instance of P2 is running. That is, after P2 has been installed
	and started, the script will ping, on the indcated port, the planetlab.py 
	script to ensure that is has indeed successfully started. If it does
	not receive a pong (after a few tries) the node is removed from the slice.
-j	Indicates the number of nodes to install and start in parallel.
-i|-I	These flags tell the script to install the rpms before starting P2. The
	'-I' installs all rpms, while the '-i' only installs the P2 rpm. When
	I say install, I also mean upload the rpm(s) from the 'phi/phi/rpms' 
	directory.		
-o	The indicated overlog script will be installed in the P2 instance after
	startup.
-D	For passing environment variables to the overlog script.
-m	A node that is designated the master node will open a channel to the
	visualizer.
-p	The port number that the master node uses to open a tcp connection
	to the visualizer.

==================================================================
Example demo session.

0. Make sure you have you ssh-agent running. If you're using a tunnel, via
a master node, to receive visualizer update messages you'll need to setup
a ssh tunnel to the master node.

$> ssh -l irb_p2 -R 10001:localhost:10001 planet2.berkeley.intel-research.net

This assumes that your master node is planet2.berkeley.intel-research.net 
(specified using the -m argument to psetup.py) with a master port 10001
(specified using the -p argument to psetup.py). Moreover, your visualizer should
have a server socket listening to port 10001 on your local machine. Deviations
from these settings should change the above command accordingly.

1. Allocate as many nodes as possible to the irb_p2 slice.

[tcondie@clash scripts]$ python psetup.py -a -n irb_p2
Allocating 580 nodes.

2. Install P2 on all nodes assigned to irb_p2, starting up a P2 instance
initialized with the gossip.olg
[tcondie@clash scripts]$ python psetup.py -t 10002 -j 20 -I -o ../../doc/gossip.olg -m planet2.berkeley.intel-research.net -p 10001 -n irb_p2

NOTE: You need not initialize a P2 instance with an overlog file. If you don't
then a plan P2 stub is all that will be running on a given planetlab node. The
gossip.olg file contains rules that gossips futher overlog installations with
other nodes in the slice.

NOTE 2: I have indicated a ping port in the previous command. This will result in
the p2setup.py script contacting the planetlab.py script on port 10002 to make
sure it is running. If for some reason this fails (the script tries 3 times in
10 second intervals) then the node is removed from the slice. Here is an example
message indicating that node planetlab-3... did not respond to the ping.
UNABLE TO GET PING FROM NODE planetlab-3.sjce.nodes.planet-lab.org!

3. Send the overlog contained in chord.olg to the master node, using p2terminal.py.
[tcondie@clash scripts]$ python p2terminal.py -t chord.olg -f chord.olg -n 1 -a planet2.berkeley.intel-research.net -p 10000 localhost 9999

NOTE: The master node is also the default landmark node in 'phi/phi/python/scripts/chord.olg'.
The master node is the first node to be installed and started in step 2, so you
may issue this command as soon as the master has been started up resulting in the
installation/startup process being overlaped with the distribution of chord.olg via
the gossip.olg algorithm. The last two arguments (localhost and 9999) are meaningless
in this context. 

NOTE 2: You should have started up the visualizer before executing the command in this
step. The installation of chord.olg will be displayed to the visualizer, along with
links and lookup responses.

4. Issue the following command to kill all P2 instances in the slice.
[tcondie@clash scripts]$ python psetup.py -j 20 -k -n irb_p2


============================================
Demo session from phi/phi:

   394  14:11   ssh -v -l irb_phi -R 10001:localhost:10001 planet2.berkeley.intel-research.net
     3  16:32   setenv PYTHONPATH python/p2/.libs/:python/dfparser/:python/dfparser/yapps/
   313  6:14    python python/scripts/psetup.py -j 10 -k -n irb_phi
   314  6:14    python python/scripts/psetup.py -DMASTER=\"planet2.berkeley.intel-research.net\" -t 10002 -j 20 -o doc/gossip.olg -m planet2.berkeley.intel-research.net -p 12221 -n irb_phi
   315  6:15    python python/scripts/p2terminal.py -DMASTER=\"planet2.berkeley.intel-research.net:10000\" -t chord.olg -f python/scripts/chord.olg -n 1 -a planet2.berkeley.intel-research.net -p 10000 localhost 9999


Set up a screen with 4 sites, tailing planetlab.out

^A S to split the screen (again and again)
^A ^I to switch among them
^A A to name them



France telecom scenario
-----------------------





As many nodes as possible

Add them all
python python/scripts/psetup.py -j 10 -a -n irb_francetelecom -u maniatis@intel-research.net -w pmPLpm ;

Kill them all
python python/scripts/psetup.py -j 10 -k -n irb_francetelecom -u maniatis@intel-research.net -w pmPLpm ;

Install them all
python python/scripts/psetup.py -I -DMASTER=\"planet1.berkeley.intel-research.net:10000\" -t 10002 -j 20 -o doc/gossip.olg -m planet1.berkeley.intel-research.net -p 12221 -n irb_francetelecom -u maniatis@intel-research.net -w pmPLpm ;

Run them all
python python/scripts/psetup.py -DMASTER=\"planet1.berkeley.intel-research.net:10000\" -t 10002 -j 20 -o doc/gossip.olg -m planet1.berkeley.intel-research.net -p 12221 -n irb_francetelecom -u maniatis@intel-research.net -w pmPLpm ;

Install chord
python python/scripts/p2terminal.py -DMASTER=\"planet1.berkeley.intel-research.net:10000\" -t chord.olg -f python/scripts/chord.olg -n 1 -a planet1.berkeley.intel-research.net -p 10000 localhost 9999



irb_p2 scenario
-----------------------

Start ssh agent and forwarder from cygwin:

eval `ssh-agent`

ssh -t -X -A maniatis@blsshsvr.berkeley.intel-research.net ssh -X -A tetryl ssh-add

ssh -v -l irb_p2 -R 12221:localhost:10001 planet2.berkeley.intel-research.net

Start the visualizer



Several reliable far apart nodes
setenv PYTHONPATH python/p2/.libs/:python/dfparser/:python/dfparser/yapps/

Kill them all
python python/scripts/psetup.py -j 10 -k -n irb_p2 -u maniatis@intel-research.net -w pmPLpm ;

Install them all
python python/scripts/psetup.py -I -DMASTER=\"planet3.berkeley.intel-research.net:10000\" -t 10002 -j 20 -o doc/gossip.olg -m planet3.berkeley.intel-research.net -p 12221 -n irb_p2 -u maniatis@intel-research.net -w pmPLpm ;

Run them all
python python/scripts/psetup.py -DMASTER=\"planet3.berkeley.intel-research.net:10000\" -t 10002 -j 20 -o doc/gossip.olg -m planet3.berkeley.intel-research.net -p 12221 -n irb_p2 -u maniatis@intel-research.net -w pmPLpm ;

Install chord
python python/scripts/p2terminal.py -DMASTER=\"planet3.berkeley.intel-research.net:10000\" -t chord.olg -f python/scripts/chord.olg -n 1 -a planet3.berkeley.intel-research.net -p 10000 localhost 9999
